{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afca66cb-81c6-46ef-afee-b1e8f209d146",
   "metadata": {},
   "source": [
    "pip install pandas\n",
    "pip install scikit-learn\n",
    "pip install xgboost\n",
    "pip install tensorflow\n",
    "pip install beautifulsoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f623748-dd58-42db-877a-cef8f09e3e90",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef86892f-5c7d-41f3-b5d2-4629d1197ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006 4783 14241\n",
      "           a+  0\n",
      "0      abound  0\n",
      "1     abounds  0\n",
      "2   abundance  0\n",
      "3    abundant  0\n",
      "4  accessable  0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv \n",
    "\n",
    "# Adding Positive Words\n",
    "def add_text(path):\n",
    "    file = open(path,'r',encoding= \"ISO-8859-1\")\n",
    "    read = file.readlines()\n",
    "    list1 = []\n",
    "    for i in read:\n",
    "        s = i.split(\"|\")\n",
    "        if(len([i]) == 1 and len(s) == 1):\n",
    "            i = i.strip()\n",
    "            a = i.replace(\"\\n\",\"\")\n",
    "            a = i.replace(\"|\",\"\")\n",
    "            list1.append(a)\n",
    "        elif (len(s) > 1):\n",
    "            a = i.replace(\"\\n\",\"\")\n",
    "            a = i.replace(\"|\",\"\")\n",
    "            a = a.split(\" \")\n",
    "            for z in a:\n",
    "                list1.append(z)\n",
    "            for i in list1:\n",
    "                if i == '\\n':\n",
    "                    list1.remove(i)\n",
    "            for i in list1:\n",
    "                if i == '':\n",
    "                    list1.remove(i)\n",
    "    file.close()\n",
    "    return list1\n",
    "\n",
    "positive_words = add_text('positive-words.txt')\n",
    "negative_words = add_text('negative-words.txt')\n",
    "\n",
    "stop_words1 = add_text('StopWords_Auditor.txt')\n",
    "stop_words2 = add_text('StopWords_Currencies.txt')\n",
    "stop_words3 = add_text('StopWords_DatesandNumbers.txt')\n",
    "stop_words4 = add_text('StopWords_Generic.txt')\n",
    "stop_words5 = add_text('StopWords_GenericLong.txt')\n",
    "stop_words6 = add_text('StopWords_Geographic.txt')\n",
    "stop_words7 = add_text('StopWords_Names.txt')\n",
    "\n",
    "stop_words1.extend(stop_words2)\n",
    "stop_words1.extend(stop_words3)\n",
    "stop_words1.extend(stop_words4)\n",
    "stop_words1.extend(stop_words5)\n",
    "stop_words1.extend(stop_words6)\n",
    "stop_words1.extend(stop_words7)\n",
    "\n",
    "print(len(positive_words),len(negative_words),len(stop_words1))\n",
    "\n",
    "for i in positive_words:\n",
    "    csvfile = open(\"words.csv\",\"a\")\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow([i,0])\n",
    "    csvfile.close()\n",
    "\n",
    "for i in negative_words:\n",
    "    csvfile = open(\"words.csv\",\"a\")\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow([i,1])\n",
    "    csvfile.close()\n",
    "\n",
    "for i in stop_words1:\n",
    "    csvfile = open(\"words.csv\",\"a\")\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow([i,2])\n",
    "    csvfile.close()\n",
    "\n",
    "data_words = pd.read_csv('words.csv')\n",
    "\n",
    "print(data_words.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a438d04-62f7-4d50-8627-ba2572f02334",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4041a2f5-0fbd-46a1-9191-4da35e703189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12873]\n",
      " [17798]\n",
      " [ 8037]\n",
      " ...\n",
      " [18083]\n",
      " [ 2253]\n",
      " [15221]]\n",
      "Accuracy: 0.9808368996671422\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.95      2007\n",
      "           1       0.97      0.98      0.98      4775\n",
      "           2       0.99      0.99      0.99     14248\n",
      "\n",
      "    accuracy                           0.98     21030\n",
      "   macro avg       0.97      0.97      0.97     21030\n",
      "weighted avg       0.98      0.98      0.98     21030\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xg\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 0 - Positive\n",
    "# 1 - Negative \n",
    "# 2 - Stop\n",
    "\n",
    "data = pd.read_csv('words.csv')\n",
    "data.columns = [\"Text\",\"Label\"]\n",
    "\n",
    "data[\"Text\"] = data[\"Text\"].apply(lambda x: str(x).lower())\n",
    "\n",
    "vocab = 20000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab, split=\" \")\n",
    "tokenizer.fit_on_texts(data[\"Text\"].values)\n",
    "X = tokenizer.texts_to_sequences(data[\"Text\"].values)\n",
    "X = pad_sequences(X,maxlen=1)\n",
    "\n",
    "Y = data[\"Label\"].values\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.20, random_state=42\n",
    ")\n",
    "\n",
    "print(X_train)\n",
    "\n",
    "model = xg.XGBClassifier(\n",
    "    objective='multi:softprob',num_class=3,n_estimators = 600,max_depth = 5)\n",
    "\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "#Making predictions on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "#Calculating accuracy\n",
    "accuracy = accuracy_score(Y_test, predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928f3190-e1eb-4cb5-a295-50ff53b2b0cc",
   "metadata": {},
   "source": [
    "## Url Text extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48895b94-7217-41aa-a556-3f35212a066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "from functools import reduce\n",
    "\n",
    "def remove_digits(lst):\n",
    "    return [reduce(lambda x, y: x+y, filter(lambda x: not x.isdigit(), s), '') for s in lst]\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "    \n",
    "\n",
    "def get_main_text(u):\n",
    "    main_t2 = []\n",
    "    main_text_final = []\n",
    "    main_t = \"\"\n",
    "    \n",
    "    response = requests.get(u)\n",
    "    html_content = response.content\n",
    "    # Parse HTML\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    texts = soup.findAll(string=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    main_t = main_t + u\" \".join(t.strip() for t in visible_texts)\n",
    "    \n",
    "    main_t = main_t.split(\".\")\n",
    "    main_t = list(filter(None, main_t))\n",
    "    for i in main_t:\n",
    "        a = i.strip().replace(\",\",\"\").replace(\"?\",\"\").replace(\":\",\"\").replace(\"+\",\"\").replace(\"/\",\"\").replace(\"!\",\"\")\n",
    "        main_t2.append(a)\n",
    "    \n",
    "    for i in main_t2:\n",
    "        main = []\n",
    "        main = i.split(\" \")\n",
    "        for i in main:\n",
    "            main_text_final.append(i)\n",
    "    \n",
    "    main_text_final = list(filter(None, main_text_final))\n",
    "    count = 0\n",
    "    for i in main_text_final:\n",
    "        if(len(i)<2):\n",
    "            main_text_final.remove(i)\n",
    "    \n",
    "    main_text_final=remove_digits(main_text_final)\n",
    "    main_text_final = list(filter(None, main_text_final))\n",
    "\n",
    "    return main_text_final,main_t2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ed9bf7-2982-4914-9e12-4ad23d003ec4",
   "metadata": {},
   "source": [
    "## Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "84e70047-f60b-476b-9c53-5b175f53001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def syllable_count(word):\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "    vowels = \"aeiouy\"\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith(\"e\"):\n",
    "        count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "def predictions(i):\n",
    "    sample = {}\n",
    "    sample.update({'lines':[i]})\n",
    "    sample = pd.DataFrame(sample)\n",
    "    \n",
    "    x_d = tokenizer.texts_to_sequences(sample['lines'].values)\n",
    "    x_d = pad_sequences(x_d,maxlen=1)\n",
    "\n",
    "\n",
    "    predictions = model.predict(x_d)\n",
    "    for i in predictions:\n",
    "        if i == 0:\n",
    "            predictions = 0\n",
    "        elif i == 1:\n",
    "            predictions = 1\n",
    "        elif i == 2:\n",
    "            predictions = 2\n",
    "            \n",
    "    return predictions\n",
    "\n",
    "\n",
    "\n",
    "def get_score(main_text_final,main_t2):\n",
    "    positive_score = 0\n",
    "    negative_score = 0\n",
    "    complex_word_count = 0\n",
    "    word_count = []\n",
    "    sum_char = 0\n",
    "    avg_word_length = 0\n",
    "    sum_syl = 0\n",
    "    \n",
    "    for i in main_text_final:\n",
    "        sum_char += len(i)\n",
    "        \n",
    "    for i in main_text_final:\n",
    "        sum_syl += syllable_count(i)\n",
    "        \n",
    "    for i in main_text_final:\n",
    "        predict = predictions(i)\n",
    "        if predict == 0:\n",
    "            word_count.append(i)\n",
    "            positive_score +=1\n",
    "        elif predict == 1:\n",
    "            word_count.append(i)\n",
    "            negative_score -= 1\n",
    "    for i in main_text_final:\n",
    "        if(syllable_count(i) > 2):\n",
    "            complex_word_count += 1\n",
    "    \n",
    "    for i in main_text_final:\n",
    "        pronounRegex = re.compile(r'\\b(I|we|my|ours|(?-i:us))\\b',re.I)\n",
    "        pronouns = pronounRegex.findall(i)\n",
    "        \n",
    "    syllable_coun = sum_syl/len(main_text_final)\n",
    "    pronouns_num = len(pronouns)\n",
    "    polarity_score = (positive_score - abs(negative_score))/((positive_score + abs(negative_score)) + 0.000001)\n",
    "    subjectivity_score = (positive_score + abs(negative_score))/ ((len(word_count)) + 0.000001)\n",
    "    avg_word_length = sum_char / len(main_text_final)\n",
    "    percentage_complex_words = complex_word_count/len(main_text_final)\n",
    "    sum_w = 0\n",
    "    for i in main_t2:\n",
    "        text_t = i.split(\" \")\n",
    "        sum_w += len(text_t)\n",
    "    avg_words_per_sentence = sum_w/len(main_t2)\n",
    "    fog = 0.4 * (avg_words_per_sentence+ percentage_complex_words)\n",
    "    \n",
    "    \n",
    "    return [positive_score,negative_score,polarity_score,subjectivity_score,avg_words_per_sentence,percentage_complex_words,fog,avg_words_per_sentence,complex_word_count,len(word_count),syllable_coun,pronouns_num,avg_word_length]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a694f1c-2881-478a-a6cd-bc4ec9c29773",
   "metadata": {},
   "source": [
    "## Output in form of CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "946d2712-ef98-4ccd-a070-fd955d236479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVERAGE NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUN</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bctech2011</td>\n",
       "      <td>https://insights.blackcoffer.com/ml-and-ai-bas...</td>\n",
       "      <td>182</td>\n",
       "      <td>-58</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.584112</td>\n",
       "      <td>0.367860</td>\n",
       "      <td>8.780789</td>\n",
       "      <td>21.584112</td>\n",
       "      <td>1243</td>\n",
       "      <td>240</td>\n",
       "      <td>2.206570</td>\n",
       "      <td>0</td>\n",
       "      <td>6.504291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bctech2012</td>\n",
       "      <td>https://insights.blackcoffer.com/streamlined-i...</td>\n",
       "      <td>66</td>\n",
       "      <td>-34</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.830769</td>\n",
       "      <td>0.349415</td>\n",
       "      <td>14.872074</td>\n",
       "      <td>36.830769</td>\n",
       "      <td>478</td>\n",
       "      <td>100</td>\n",
       "      <td>2.205409</td>\n",
       "      <td>0</td>\n",
       "      <td>6.590643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bctech2013</td>\n",
       "      <td>https://insights.blackcoffer.com/efficient-dat...</td>\n",
       "      <td>66</td>\n",
       "      <td>-58</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45.870370</td>\n",
       "      <td>0.316973</td>\n",
       "      <td>18.474938</td>\n",
       "      <td>45.870370</td>\n",
       "      <td>465</td>\n",
       "      <td>124</td>\n",
       "      <td>2.145194</td>\n",
       "      <td>0</td>\n",
       "      <td>6.426721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bctech2014</td>\n",
       "      <td>https://insights.blackcoffer.com/effective-man...</td>\n",
       "      <td>66</td>\n",
       "      <td>-47</td>\n",
       "      <td>0.168142</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.268657</td>\n",
       "      <td>0.340237</td>\n",
       "      <td>14.243557</td>\n",
       "      <td>35.268657</td>\n",
       "      <td>460</td>\n",
       "      <td>113</td>\n",
       "      <td>2.166420</td>\n",
       "      <td>0</td>\n",
       "      <td>6.467456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bctech2015</td>\n",
       "      <td>https://insights.blackcoffer.com/streamlined-t...</td>\n",
       "      <td>61</td>\n",
       "      <td>-42</td>\n",
       "      <td>0.184466</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56.340909</td>\n",
       "      <td>0.325982</td>\n",
       "      <td>22.666756</td>\n",
       "      <td>56.340909</td>\n",
       "      <td>473</td>\n",
       "      <td>103</td>\n",
       "      <td>2.128877</td>\n",
       "      <td>0</td>\n",
       "      <td>6.274983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       URL_ID                                                URL  \\\n",
       "0  bctech2011  https://insights.blackcoffer.com/ml-and-ai-bas...   \n",
       "1  bctech2012  https://insights.blackcoffer.com/streamlined-i...   \n",
       "2  bctech2013  https://insights.blackcoffer.com/efficient-dat...   \n",
       "3  bctech2014  https://insights.blackcoffer.com/effective-man...   \n",
       "4  bctech2015  https://insights.blackcoffer.com/streamlined-t...   \n",
       "\n",
       "   POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0             182             -58        0.516667                 1.0   \n",
       "1              66             -34        0.320000                 1.0   \n",
       "2              66             -58        0.064516                 1.0   \n",
       "3              66             -47        0.168142                 1.0   \n",
       "4              61             -42        0.184466                 1.0   \n",
       "\n",
       "   AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0            21.584112                     0.367860   8.780789   \n",
       "1            36.830769                     0.349415  14.872074   \n",
       "2            45.870370                     0.316973  18.474938   \n",
       "3            35.268657                     0.340237  14.243557   \n",
       "4            56.340909                     0.325982  22.666756   \n",
       "\n",
       "   AVERAGE NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                             21.584112                1243         240   \n",
       "1                             36.830769                 478         100   \n",
       "2                             45.870370                 465         124   \n",
       "3                             35.268657                 460         113   \n",
       "4                             56.340909                 473         103   \n",
       "\n",
       "   SYLLABLE PER WORD  PERSONAL PRONOUN  AVG WORD LENGTH  \n",
       "0           2.206570                 0         6.504291  \n",
       "1           2.205409                 0         6.590643  \n",
       "2           2.145194                 0         6.426721  \n",
       "3           2.166420                 0         6.467456  \n",
       "4           2.128877                 0         6.274983  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csvfile = open(\"Input.csv\",\"r\")\n",
    "csvreader = csv.reader(csvfile)\n",
    "\n",
    "csvout = open(\"Output.csv\",\"a\")\n",
    "csvwriter = csv.writer(csvout)\n",
    "csvwriter.writerow(['URL_ID','URL','POSITIVE SCORE','NEGATIVE SCORE','POLARITY SCORE','SUBJECTIVITY SCORE','AVG SENTENCE LENGTH','PERCENTAGE OF COMPLEX WORDS','FOG INDEX','AVERAGE NUMBER OF WORDS PER SENTENCE','COMPLEX WORD COUNT','WORD COUNT','SYLLABLE PER WORD','PERSONAL PRONOUN','AVG WORD LENGTH'])\n",
    "\n",
    "names = []\n",
    "link = []\n",
    "for lines in csvreader:\n",
    "    names.append(lines[0])\n",
    "    link.append(lines[1])\n",
    "\n",
    "names = names[1:]\n",
    "link = link[1:]\n",
    "\n",
    "\n",
    "for i in range(0,len(link)):\n",
    "    print(i)\n",
    "    x,y = get_main_text(link[i])\n",
    "    csvwriter.writerow([names[i],link[i],get_score(x,y)[0],get_score(x,y)[1],get_score(x,y)[2],get_score(x,y)[3],get_score(x,y)[4],get_score(x,y)[5],get_score(x,y)[6],get_score(x,y)[7],get_score(x,y)[8],get_score(x,y)[9],get_score(x,y)[10],get_score(x,y)[11],get_score(x,y)[12]])\n",
    "\n",
    "csvfile.close()\n",
    "csvout.close()\n",
    "\n",
    "data = pd.read_csv('Output.csv',on_bad_lines='skip')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cc0c85-1a32-4611-afda-37b091a3eda6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
